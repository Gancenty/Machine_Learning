{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import re\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36019\n"
     ]
    }
   ],
   "source": [
    "def read_time_machine():\n",
    "    with open('./time_machine.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    format_lines = [re.sub('[^A-Za-z]', ' ', line).strip().lower() for line in lines]\n",
    "    lines = [line for line in format_lines if len(line)>0]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def tokenizer(data,method='word'):\n",
    "    ret = []\n",
    "    if method=='word':\n",
    "        for line in data:\n",
    "            ret.extend(line.split())\n",
    "        return ret\n",
    "    elif method=='char':\n",
    "        for line in data:\n",
    "            ret.extend([i for i in line if i.isalnum()])\n",
    "        ret = sorted(list(set(ret)))\n",
    "        return ret\n",
    "\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "time_machine_token = tokenizer(read_time_machine(),'word')\n",
    "print(len(time_machine_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self,tokens,min_frequence=0,reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = count_corpus(tokens)\n",
    "        self.__token_freq = sorted(counter.items(),key=lambda x:x[1],reverse=True)\n",
    "        self.index_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.unk = 0\n",
    "        self.token_to_index = {token:idx for idx,token in enumerate(self.index_to_token)}\n",
    "        for token,freq in self.token_freq:\n",
    "            if freq <min_frequence:\n",
    "                break\n",
    "            if token not in self.index_to_token:\n",
    "                self.index_to_token.append(token)\n",
    "                self.token_to_index[token] = len(self.index_to_token) - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index_to_token)\n",
    "    \n",
    "    def to_index(self,tokens):\n",
    "        if not isinstance(tokens,(list,tuple)):\n",
    "            return self.token_to_index.get(tokens,self.unk)\n",
    "        return [self.to_index(token) for token in tokens]\n",
    "\n",
    "    def to_token(self,indices):\n",
    "        if not isinstance(indices,(tuple,list)):\n",
    "            return self.index_to_token[indices]\n",
    "        return [self.to_token(index) for index in indices]\n",
    "\n",
    "    @property\n",
    "    def token_freq(self):\n",
    "        return self.__token_freq\n",
    "\n",
    "time_machine_corpus = Vocab(time_machine_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paired_token = [pair for pair in zip(token[:-1],token[1:])]\n",
    "# paired_corpus = Vocab(paired_token)\n",
    "# for i in range(10):\n",
    "#     print(paired_corpus.token_freq[i])\n",
    "\n",
    "# tri_token = [pair for pair in zip(token[:-2],token[1:-1],token[2:])]\n",
    "# tri_corpus = Vocab(tri_token)\n",
    "# for i in range(10):\n",
    "#     print(tri_corpus.token_freq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,axes = plt.subplots(2,2)\n",
    "# x = [i for i in range(len(time_machine_corpus)-1)]\n",
    "# y = [time_machine_corpus.token_freq[j][1] for j in x]\n",
    "# print(y)\n",
    "# print(time_machine_corpus.token_freq)\n",
    "# axes[0][0].plot(x,x)\n",
    "# axes[0][1].plot(x,y)\n",
    "# axes[1][1].plot(x,y)\n",
    "# axes[1][1].semilogx()\n",
    "# axes[1][1].semilogy()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    num_subseqs = (len(corpus)-1) // num_steps\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        return corpus[pos: pos + num_steps]\n",
    "    \n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    offset = random.randint(0, num_steps - 1)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'seemed', 'to', 'me', 'of', 'a', 'very', 'great', 'depth', 'one']] \n",
      " [['seemed', 'to', 'me', 'of', 'a', 'very', 'great', 'depth', 'one', 'lay']]\n",
      "\n",
      "[['it'], ['seemed'], ['to'], ['me'], ['of'], ['a'], ['very'], ['great'], ['depth'], ['one']] \n",
      " [['seemed'], ['to'], ['me'], ['of'], ['a'], ['very'], ['great'], ['depth'], ['one'], ['lay']]\n",
      "torch.Size([1, 10]) \n",
      " torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class SeqDataLoader:\n",
    "    def __init__(self, token ,batch_size, num_steps, use_random_iter):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "        self.token_index = time_machine_corpus.to_index(token)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.token_index, self.batch_size, self.num_steps)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_index)\n",
    "    \n",
    "data_iter = SeqDataLoader(time_machine_token,batch_size=1,num_steps=10,use_random_iter=True)\n",
    "for i,j in data_iter:\n",
    "    print(time_machine_corpus.to_token(i.detach().numpy().tolist()),'\\n',time_machine_corpus.to_token(j.detach().numpy().tolist()))\n",
    "    print()\n",
    "    print(time_machine_corpus.to_token(i.T.detach().numpy().tolist()),'\\n',time_machine_corpus.to_token(j.T.detach().numpy().tolist()))\n",
    "    print(i.shape,'\\n',j.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))\n",
    "        \n",
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(time_machine_corpus), num_hiddens)\n",
    "rnn_net = RNNModel(rnn_layer=rnn_layer,vocab_size=len(time_machine_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix, num_preds, net, vocab, device):\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab.to_index(prefix[0])]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab.to_index(y))\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ' '.join([vocab.to_token(i) for i in outputs])\n",
    "\n",
    "\n",
    "def train_epoch(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    state =  None\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "\n",
    "        updater.zero_grad()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        updater.step()\n",
    "\n",
    "\n",
    "def trainer(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    predict = lambda prefix: predict(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        bar = tqdm(train_iter,total=len(train_iter))\n",
    "        train_epoch(net, bar, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict(['time','traveller']))\n",
    "    print(predict(['time','traveller']))\n",
    "    print(predict(['traveller']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3308/36019 [00:34<05:43, 95.19it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer(rnn_net, data_iter,\u001b[38;5;28mlen\u001b[39m(time_machine_corpus),\u001b[38;5;241m0.01\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[21], line 45\u001b[0m, in \u001b[0;36mtrainer\u001b[1;34m(net, train_iter, vocab, lr, num_epochs, device, use_random_iter)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     44\u001b[0m     bar \u001b[38;5;241m=\u001b[39m tqdm(train_iter,total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_iter))\n\u001b[1;32m---> 45\u001b[0m     train_epoch(net, bar, loss, updater, device, use_random_iter)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28mprint\u001b[39m(predict([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraveller\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[21], line 29\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(net, train_iter, loss, updater, device, use_random_iter)\u001b[0m\n\u001b[0;32m     27\u001b[0m y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 29\u001b[0m y_hat, state \u001b[38;5;241m=\u001b[39m net(X, state)\n\u001b[0;32m     30\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(y_hat, y\u001b[38;5;241m.\u001b[39mlong())\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     32\u001b[0m updater\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Aian\\.conda\\envs\\pytorch_cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aian\\.conda\\envs\\pytorch_cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[1;34m(self, inputs, state)\u001b[0m\n\u001b[0;32m     15\u001b[0m X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(inputs\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mlong(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)\n\u001b[0;32m     16\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 17\u001b[0m Y, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(X, state)\n\u001b[0;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(Y\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, state\n",
      "File \u001b[1;32mc:\\Users\\Aian\\.conda\\envs\\pytorch_cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aian\\.conda\\envs\\pytorch_cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aian\\.conda\\envs\\pytorch_cpu\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:553\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 553\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_tanh(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    554\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m    555\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    558\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m    559\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer(rnn_net, data_iter,len(time_machine_corpus),0.01,100,'cpu',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
