{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "\n",
    "def parse_arg():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--lr\",type=float,default=0.078)\n",
    "    parser.add_argument(\"--seed\",type=int,default=5319)\n",
    "    parser.add_argument(\"--batchsize\",type=int,default=256)\n",
    "\n",
    "    parser.add_argument(\"--dataset_dir\",type=str,default=r\"./DATASETS/classify_leaves\")\n",
    "    parser.add_argument(\"--checkpoint_dir\",type=str,default=r\"./CHECK_POINT\")\n",
    "    parser.add_argument(\"--checkpoint_name\",type=str,default=r\"classify_leaves.pt\")\n",
    "    \n",
    "    parser.add_argument(\"--epoch\",type=int,default=25)\n",
    "    parser.add_argument(\"--device\",type=str,default=\"cuda\")\n",
    "    \n",
    "    arg = parser.parse_args(args=[])\n",
    "    return arg\n",
    "\n",
    "args = parse_arg()\n",
    "checkpoint_path = args.checkpoint_dir + '/' + args.checkpoint_name\n",
    "device = torch.device(args.device)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaves_Dataset(data.Dataset):\n",
    "    def __init__(self,path,mode) -> None:\n",
    "        super().__init__()\n",
    "        self.mode = mode       \n",
    "        self.ratio = 4/5\n",
    "        if self.mode not in ['train','dev','test']:\n",
    "            raise ValueError\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            with open(path,'r') as fp:\n",
    "                self.all_sample = pd.read_csv(fp)\n",
    "                self.all_sample_name  = self.all_sample['image']\n",
    "                self.all_sample_label = self.all_sample['label']\n",
    "\n",
    "                self.label_class = sorted(set(self.all_sample_label))\n",
    "                self.onehot = torch.zeros(len(self.label_class))\n",
    "                self.len = len(self.all_sample_name)\n",
    "                \n",
    "                self.train_len = int(self.ratio*self.len)\n",
    "                self.dev_len   = self.len - self.train_len\n",
    "\n",
    "                self.train_name  = self.all_sample_name[0:self.train_len]\n",
    "                self.train_label = self.all_sample_label[0:self.train_len]\n",
    "\n",
    "                self.dev_name = self.all_sample_name[self.train_len:self.len].reset_index(drop=True)\n",
    "                self.dev_label = self.all_sample_label[self.train_len:self.len].reset_index(drop=True)\n",
    "        else:\n",
    "            with open(path,'r') as fp:\n",
    "                self.test = pd.read_csv(fp)\n",
    "                self.test_name  = self.test['image']\n",
    "                self.len = len(self.test_name)\n",
    "        self.train_trans = transforms.Compose(\n",
    "            [\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.RandomVerticalFlip(),\n",
    "             ]\n",
    "        )    \n",
    "        self.trans = transforms.Compose(\n",
    "            [\n",
    "                        transforms.ToTensor(),\n",
    "             ]\n",
    "        )\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            # For training\n",
    "            if self.mode == 'train':\n",
    "                file_name = args.dataset_dir+'/'+self.train_name[index]\n",
    "                image = Image.open(file_name)\n",
    "                data  = self.train_trans(image)\n",
    "                label_embed = self.onehot.clone()\n",
    "                label_embed[self.label_class.index(self.train_label[index])] = 1\n",
    "                return data, label_embed\n",
    "            if self.mode == 'dev':\n",
    "                file_name = args.dataset_dir+'/'+self.dev_name[index]\n",
    "                image = Image.open(file_name)\n",
    "                data  = self.trans(image)\n",
    "                label_embed = self.onehot.clone()\n",
    "                label_embed[self.label_class.index(self.dev_label[index])] = 1\n",
    "                return data, label_embed\n",
    "        else:\n",
    "            # For testing (no target)\n",
    "            file_name = args.dataset_dir+'/'+self.test_name[index]\n",
    "            image = Image.open(file_name)\n",
    "            data  = self.trans(image)\n",
    "            return data,self.test_name[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the size of the dataset\n",
    "        if self.mode in ['train','dev']:\n",
    "            if self.mode == 'train':\n",
    "                return self.train_len\n",
    "            if self.mode == 'dev':\n",
    "                return self.dev_len\n",
    "        else:\n",
    "            return self.len\n",
    "\n",
    "train_dataset    = Leaves_Dataset(args.dataset_dir+'/train.csv',mode='train')\n",
    "train_dataloader = data.DataLoader(train_dataset,args.batchsize,shuffle=True)\n",
    "\n",
    "dev_dataset    = Leaves_Dataset(args.dataset_dir+'/train.csv',mode='dev')\n",
    "dev_dataloader = data.DataLoader(dev_dataset,args.batchsize,shuffle=False)\n",
    "\n",
    "test_dataset    = Leaves_Dataset(args.dataset_dir+'/test.csv',mode='test')\n",
    "test_dataloader = data.DataLoader(test_dataset,args.batchsize,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIcAAACHCAYAAAA850oKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAN80lEQVR4nO2d348lRRXHv+dU9+zMwIIEwgArGyJxF4WVXzGGB+OLT8YHTTTqk8Q3n4j/hS/yrkQTfdDE/8AHEyUYMAbJxsgPYZFVfrrggsyyM7e76/hQVd1VfbvudA/D3rkz55PMj9td3V0/vnXqnOrb1SQiAkUZgJedAeXwouJQsqg4lCwqDiWLikPJouJQsqg4lCwqDiWLikPJUoxNqBOpRwsi2jONWg4li4pDyaLiULKoOJQsKg4li4pDyTI6lH300R8A6MJZEUFd1+giIoGIwFoLok5z1lofBkt7XAiL43Aq3t5tswAougbBWpk7biwS5R/kjp0L6DKnE8DlQ+a3JycRzJ9zDgbQlWN9Yx2PPfYjnHvgAVRVDdtYWLFpnUiafxEZyIuEos0Xo7fh9ltv3jOXo8Xx1FNPQsRVUBBBVc3AHKqCYa1FXddgduIgIjRN4wVDbUH7QnD7bCuGtjxi22u67QRr7VzeRGQubh8UDcX/uvzIkBqGBEKZ/ZRupoHjUzGHbd2+jc0NfPs738W9cj8aK6h9nYWOFZcl93/8Oa7rXNoxTJoEi+ufCCBiEMj3QgBMIOa2wtpe5X/I74gbM/wVIYRRjvz5iEw4i08n/rpxvuK8uQsNiSUHDfXzvQ7N7B9zyfk0ArGCarYLwwxmAjO7khBFljeU01mdXBmHrHK8fQqjxQGwV2RoPPZ1RCABQBYk1jWeFwzaXkJ+myDOY19sca0HC0VE0b4gjrQbd9aL2vT7ndAdqsQ4D8P7gzX0eYvMR2op+uYnNLBFVVUgCAwThAmNONkGKxxfNh6mc3kes28vRouDKCgSiBvBDbKu4ZgAG6UFCQgCwDqLIi49c/+8BDda9Iea8EkioVDWKlDUKLEv1CmV/LmiaxMlw8BwXYZGTK1nfF0RgiCUN7ZgGMgzw9WU/8SMuqqj87GrI58ZNywjOScGrPBgziVvZfZitDhuvOmku5C1PpMWdW0AAEyMIJimacAUrETXO4h8tVHnN8RDSyhEahYpahQCgWFtt3/+B2BDYDYwxvh8oE3PzP6vATMl2zrrlpabIyWHdMwMY9w1iqIAM6MsS2xsbGBtba39KcsSZVGiKAsYU6AoDIwpADFRZ3ND8YMPPRhdQ0BMYBkaNubdzU/KctDYRxOe/8cLaGwDaZpoDGwAOHEwFTBFASZGURhX6UwoCoOyLN024iiScT3M9bYgkvSasfPpGs+J0UoXEVFyoG3H6qFIiDgSgPf+OeQnNi756kJrmUCR6N2B4fydVUWXz2hoskKA2PZS1loIsXdGnSNqQx0DkMgHn+po5vbfetMNC48DJliOra272gK6ziwgEudCEgDidnjpPnZmkL0j0HdE04bMt8wYs+htzUAk1A0189HJYODXEuvFjYqdmMO2+Jydgx1tQyepKKGLztoY2dVcsEwQ212Phx3tvZzSOD/7YbQ4ZlXT9jqOTTm8A8ouwI/FAz8GB1PJiVPZjpoZoaSFC+Fw2CahkSLHtQv943Cm+2y7uLjH4grsS2dgdqTb1ybuGlOQCUGjP2HOhYlgIaBoLiT4n62VkrSDzefhYIaZ0eKwdtaOyxYG3oACcKErWYAZfq4CXQRB4k24K1MQDhM5a9LLODMnzme/UpP5EkjacpFD6hO0n/bff3oMWIb4c9xg/f/nGiYMaUlHcnlmP+BG/q0/PlRi6FpILHIsnj4h7VgmhLIhQ6FABBB3lU9dujTSJF8eGt4PZMUwxCTPOxoxosBn3HETGBLwFPrpGYC0IkwzFVtJf8VehsO+/nA5yqlKmCQON+nlJ75CJkLBgoMXh6mIogq/oQ3teuNi3xdJrjzkXA5YncHjE4dhcdW09iianxlDLO5FLJr2jztHW1+9GVwJQzLioW3gmu18UiyQbt5lrHYnzHO4UDKEmOHSrrHTuYVWAN5dYy8i79ODKa2oISc1ZmhGNf5/ztr4DpKIaLHf2SaZn5zf65jFcy99cpYxFgbHM2fJeSNrQvnCpOKmZOiZMrSMFkcX78e93AnDqTlqQAAE2/ooTPFcQaf1fqXm/h9iYVrqtnc5mu+pQK+xCEkkMvb6cflyZC1b5txZ4SPoJe+MUlT+7hxdTYwdWiZYDiSndQ6neHF4ETAlaZnd1HrS89uIZdhy9P2PNp20vxYUNN7fzUe024YscGzqMbbaJM3OCNp8ZC6Q81Xm6iFzbEg7Jv1YJonDWgsQYLiE00FogDTMJL8vDDvwPZiIfcxuk5nHcFzsT8wVkACKQjgZakrpek57/MevowHCkLkPQuS28PjUUZpiRdvLZIbiKUxwSJ0lMGzakMtdGG5m0LcDtTdl/YRO2/OjIYc5sQgiFo2fx+Bkn3hhhYtFs6mR9+7y4Zqrqqr2awKbm5tt3vfquXvsGk49sc7bIWLR8CVoQ1gn7r1F2L9Vn0785Z3gvZgmDgKIvMtG3WwpEcEw0nkL4sTxCROoIT2AbsgBoTQmKm3X2N39suFwI7Y4AFAUhbuvEovskBDKFNtM6ZfL+28yIe+LnPtcujGMd0ipM1PuOwfUTloRef8C88ND/2fuvAuGlzisG9J83EtiM2q80JLec0h00s9GV0ZvCefSHUTG9zcETohWencwCa0wgC4y6E9m9YURRzSDxZjo1ffnGOJrh9nWqee9llyb/HzCPocx8TDSicENJeKHnPT2doB9vN2NtdKNqZ4hD3tMaDvFfObG32UKZq7MS8rHENMmwbyDGf+lIA523+lIjkHfEaTEfeif/yCIRXZYBLCqjLcczOnd2MSUDzt/7RfCFliEg2bsRJqKZW8miQPAXLg5KIoQbmr9rzSTh5Voi7851N1UWzyxo6wak+6tpOJIvxG1OAZRVpFJliP5G904bNMcXL6UQ8CkGdIEGnv/UllVplsO/1uFcfQZL44DntBVDj+Tl2BQYRwfJlgO5bihi7coWVQcShYVh5JFxaFkUXEoWVQcShYVh5JFxaFkUXEoWfZ/V/ZQEq/N0X/Apf9VJPfUWvQt094jl/0vJMM/SXd8vuF2xMQBLH5GIzxDz8Pb/PMj/ccZ3NN7k9Y9ORIcMXGQtxgCkIVreP+MmRgQTPctJW8l2oeJyD/BKkOLyPTXuzgepuOIiSMsj02AGIAIEm0L33kFiX8WM3rORpygrDgh1HWDy5cvo65rbG1toSiKyc+arjpHUhwC0z2QDMBKBZEKYoErH22DSHDy5EkQ+TVBvXhsLXjnP2/j4sWL2N7exm233QEA2NraumbCGHrW5qCWVJjK6HVIV6HXhEVxBYKd3Y/w/Evn8dz5p7E7u4qmaTDbrfDOpbdBxDh16k6cvvMz+MqXv4obNm/CBx9s45WXX8F1mxs4e/asX+i26zvXqqFEBFVVgZmTx08PmlGPmx4tcQCNrfH+lbfw698+gefO/wkWMzRNhaZ2jmddNyAUsMIgKvCZuz6Lh+9/BKdPncED576IE2vrAA52EZRpZcgvC3WQHCtxiAgEFm9euoif//JxvPbG39BgB2wMbG1hG4u6qmAbC6AEUwmAsHN1F7fdcho/+fFPsXniFgw5m8sSyNiF6PbDmHOujM8Rr6lH8YYwPQHBu5ffxBO/eBwvXngWxXoDUxiQrDknEw2K0kDIoqkN6krALFgrDArDgGWQsI9aOpZpOeJVApbByogjpVsCRaSCoAFQ4I9P/w4v/fOvKE4wIISmEtSyA8PGre1pGU1j0cgVsNmEoTVYqlCurcOU5dKFsaxr5lhBcUQ2RACgBKjA7myG119/DUADsgWIDQD3hoamAWxjQWAQF1hfux51A8DWgAXuueccDJ9oz3uYGmiZeVkhccSrA7qohMitEw6q8N77r+KFF59FYdw667ANmA3qxr3H5OT11wEg7MxqNFKhsRVKQyh4Aw/f/wiKSByKY2XE0S2KFBaCcbOZgBPIM395EldnH4IZsLaB+PXGTMEwhlGuMXZ2Z4CxoKbBWsEQa/CF+76Ec/c9DH1Qa56VEUdLmOF0HyAArnz0If7+4nmIFbApQSwoCjcR1tgGAkEj1s+KzsAkgJS4fesMvvWNR1EWJ+BeaqI3qWNWRhzBSnS9uwEIePfSW/jVb36GCxdegDENTOmmwoncxDmIwLyGpilQVzUYBWwD3PypT+P73/shTt9xxp+zQXjph+JYmXmO+C0B4b1ws9kML7/yEhrZQXmC8b/t/+KZPz+Ff71+AR9sv4e6uYqq3vGrEhmIJZR8Az5/9iF8/WvfxN133QvGCYAad6NOSgDH45b8kZoE665u3erBYrxbWjkrIeRupJHFO5f+jTfefBV/ePL3uLp7BYUpcffdZ3DPmbO4bv1GnD71OZTlGkQsCIW/EWcB6V6FetQ5ouKIJsHIrX7sfBD/EkKE11D4tUwBkDBA7ERFTljtudpKOj634oEjJo79MPzGgeMjgEUcqenz/aBC+Hho7KZkWZrlGHqNlXK4WIrlGHoXSPzmA+VwsLRhRYVw+FmqOA7rmwwUx1J8jvjbTSqQw8vSQ1kVxeFFQ1kli4pDyaLiULKoOJQsKg4li4pDyaLiULKoOJQsKg4li4pDyaLiULKoOJQsKg4li4pDyaLiULKoOJQsKg4li4pDyaLiULKoOJQsKg4li4pDyaLiULKoOJQsKg4li4pDyaLiULKoOJQsKg4li4pDyaLiULKoOJQsKg4li4pDyaLiULKoOJQsKg4li4pDyaLiULKoOJQsoxep1cVkjx9qOZQsKg4li4pDyaLiULKoOJQsKg4li4pDyaLiULKoOJQs/wdrwzmxFfFXygAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_img(img,label,nrow,ncol,scale=1.5):\n",
    "    fig,axes = plt.subplots(nrow,ncol, figsize=(ncol*scale,nrow*scale)) \n",
    "    if nrow*ncol == 1:\n",
    "        axes = [axes, ]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    for i,(img,ax) in enumerate(zip(img,axes)):\n",
    "        ax.imshow(img.squeeze().permute(1,2,0))\n",
    "        ax.set_title('')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "x,y = next(iter(train_dataset))\n",
    "print(x.shape)\n",
    "show_img(x.unsqueeze(0),y,1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride=1, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1,stride=stride)\n",
    "        self.conv2d_2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1,stride=1)\n",
    "        self.batch_normal_1 = nn.BatchNorm2d(out_channels)\n",
    "        self.batch_normal_2 = nn.BatchNorm2d(out_channels)\n",
    "        self.res_conv = nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride)\n",
    "        self.using_conv1x1 = in_channels != out_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "    def forward(self,x):\n",
    "        y = F.relu(self.batch_normal_1(self.conv2d_1(x)))\n",
    "        y = self.batch_normal_2(self.conv2d_2(y))\n",
    "        if self.using_conv1x1:\n",
    "            x = self.res_conv(x)\n",
    "        y = y+x\n",
    "        return F.relu(y)\n",
    "    \n",
    "b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(ResNet(input_channels, num_channels,\n",
    "                                stride=2))\n",
    "        else:\n",
    "            blk.append(ResNet(num_channels, num_channels))\n",
    "    return blk\n",
    "\n",
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
    "\n",
    "net_layer = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "                    nn.Flatten(), nn.Linear(512, 176))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.net = net_layer\n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(y_pred,y):\n",
    "    y_pred = y_pred.softmax(1).argmax(1)\n",
    "    y = y.argmax(1)\n",
    "    right = (y_pred == y).sum()\n",
    "    return (right,y.shape[0])\n",
    "\n",
    "def train(train_loader,dev_loader,model,n_epoch,lr,device):\n",
    "    model.to(device)\n",
    "    if os.path.exists(path=checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path,map_location=device)\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print('find the last checkpoint,load successfully')\n",
    "    else:\n",
    "        print('not find the checkpoint')\n",
    "    optim = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    max_acc = 0\n",
    "    for epoch in range(n_epoch):\n",
    "        bar = tqdm(train_loader,total=len(train_loader),unit_scale=args.batchsize)\n",
    "        model.train()\n",
    "        for x,y in bar:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred,y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            bar.set_description(f'Epoch:[{epoch+1:02d}/{n_epoch}]')\n",
    "            bar.set_postfix(loss=float(loss))\n",
    "        right, total = 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x,y in dev_loader:\n",
    "                model.to(device)\n",
    "                x,y = x.to(device),y.to(device)\n",
    "                pred = model(x)\n",
    "                a,b = accuracy(pred,y)\n",
    "                right += a\n",
    "                total += b\n",
    "        acc = right/total*100\n",
    "        print(\"Acc:%.2f%%\"%(acc))\n",
    "        if acc > max_acc:\n",
    "            torch.save(net.state_dict(),checkpoint_path)\n",
    "            max_acc = acc\n",
    "            print(\"Saving Model\")\n",
    "        else:\n",
    "            print(\"No improving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find the last checkpoint,load successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[01/25]: 100%|██████████| 14848/14848 [00:43<00:00, 337.80it/s, loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:49.88%\n",
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[02/25]: 100%|██████████| 14848/14848 [00:41<00:00, 357.24it/s, loss=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:16.59%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[03/25]: 100%|██████████| 14848/14848 [00:41<00:00, 359.66it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:49.47%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[04/25]: 100%|██████████| 14848/14848 [00:41<00:00, 359.53it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:18.77%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[05/25]: 100%|██████████| 14848/14848 [00:41<00:00, 360.38it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:60.23%\n",
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[06/25]: 100%|██████████| 14848/14848 [00:41<00:00, 359.91it/s, loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:1.55%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[07/25]: 100%|██████████| 14848/14848 [00:41<00:00, 360.08it/s, loss=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:15.80%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[08/25]: 100%|██████████| 14848/14848 [00:41<00:00, 360.83it/s, loss=0.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:15.88%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[09/25]: 100%|██████████| 14848/14848 [00:41<00:00, 360.20it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:43.67%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[10/25]: 100%|██████████| 14848/14848 [00:41<00:00, 360.29it/s, loss=0.152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:48.38%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[11/25]: 100%|██████████| 14848/14848 [00:41<00:00, 358.17it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:24.49%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[12/25]: 100%|██████████| 14848/14848 [00:41<00:00, 358.82it/s, loss=0.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:65.05%\n",
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[13/25]: 100%|██████████| 14848/14848 [00:40<00:00, 366.31it/s, loss=0.0965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:76.76%\n",
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[14/25]: 100%|██████████| 14848/14848 [00:40<00:00, 366.03it/s, loss=0.0889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:54.10%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[15/25]: 100%|██████████| 14848/14848 [00:40<00:00, 366.89it/s, loss=0.0623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:49.69%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[16/25]: 100%|██████████| 14848/14848 [00:40<00:00, 367.52it/s, loss=0.0885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:30.54%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[17/25]: 100%|██████████| 14848/14848 [00:40<00:00, 366.80it/s, loss=0.0433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:86.65%\n",
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[18/25]: 100%|██████████| 14848/14848 [00:40<00:00, 367.14it/s, loss=0.0619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:60.58%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[19/25]: 100%|██████████| 14848/14848 [00:40<00:00, 367.33it/s, loss=0.083] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:78.48%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[20/25]: 100%|██████████| 14848/14848 [00:40<00:00, 367.20it/s, loss=0.0776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:81.72%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[21/25]: 100%|██████████| 14848/14848 [00:40<00:00, 366.89it/s, loss=0.041] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:84.69%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[22/25]: 100%|██████████| 14848/14848 [00:40<00:00, 367.11it/s, loss=0.0586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:82.38%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[23/25]: 100%|██████████| 14848/14848 [00:40<00:00, 366.11it/s, loss=0.0509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:81.29%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[24/25]: 100%|██████████| 14848/14848 [00:40<00:00, 367.32it/s, loss=0.0531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:74.61%\n",
      "No improving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[25/25]: 100%|██████████| 14848/14848 [00:40<00:00, 366.49it/s, loss=0.0262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:87.20%\n",
      "Saving Model\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader,dev_dataloader,net,args.epoch,args.lr,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:97.14%\n"
     ]
    }
   ],
   "source": [
    "right, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for x,y in train_dataloader:\n",
    "        net.to(args.device)\n",
    "        x,y = x.to(args.device),y.to(args.device)\n",
    "        pred = net(x)\n",
    "        a,b = accuracy(pred,y)\n",
    "        right += a\n",
    "        total += b\n",
    "print(\"Acc:%.2f%%\"%(right/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8960/8960 [00:14<00:00, 608.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(test_loader,model,device):\n",
    "    model.to(device)\n",
    "    bar = tqdm(test_loader,total=len(test_loader),unit_scale=args.batchsize)\n",
    "    model.eval()\n",
    "    ans = []\n",
    "    for x,y in bar:\n",
    "        x = x.to(device)\n",
    "        file_name = y\n",
    "        pred = model(x)\n",
    "        pred = pred.argmax(1)\n",
    "        pred_ans = [[name,train_dataset.label_class[target]] for name,target in zip(file_name,pred)]\n",
    "        ans.extend(pred_ans)\n",
    "    output = pd.DataFrame(ans,columns=['image','label'])\n",
    "    output.to_csv('ans.csv',index=False)\n",
    "\n",
    "predict(test_dataloader,net,args.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
